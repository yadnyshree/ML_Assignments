# -*- coding: utf-8 -*-
"""clickbait.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_Z0390CdvV1o-aQOu__pXVB3Q8eKY-5O
"""

# -*- coding: utf-8 -*-
"""Clickbait.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wM1QDYqTSIsDNHn6W8KJULJtL-pJeli1

Clickbait
"""

# Commented out IPython magic to ensure Python compatibility.
from sklearn.feature_extraction.text import TfidfVectorizer
import cloudpickle
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
import gensim
from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import STOPWORDS
from nltk.stem import WordNetLemmatizer, SnowballStemmer
from gensim.models.word2vec import Word2Vec
from gensim.models.doc2vec import TaggedDocument
from sklearn.model_selection import train_test_split
import warnings
import numpy as np
np.random.seed(2018)
import nltk
nltk.download('wordnet')
import re
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import nltk
import pickle
nltk.download('stopwords')
from nltk.corpus import stopwords 
from nltk.stem.porter import PorterStemmer
from nltk.tokenize import word_tokenize 
nltk.download('punkt')
import nltk
from scipy import sparse
from urllib.request import urlopen
nltk.download('averaged_perceptron_tagger')
from string import punctuation

class ClickBait():

  def tokenization(self,text):
    lst=text.split()
    return lst

  def lowercasing(self,lst):
      new_lst=[]
      for i in lst:
          i=i.lower()
          new_lst.append(i)
      return new_lst
 
  def remove_punctuations(self,lst):
      new_lst=[]
      for i in lst:
          for j in punctuation:
              i=i.replace(j,'')
          new_lst.append(i)
      return new_lst


  def remove_numbers(self,lst):
      nodig_lst=[]
      new_lst=[]
      for i in lst:
          for j in self.digits:    
              i=i.replace(j,'')
          nodig_lst.append(i)
      for i in nodig_lst:
          if i!='':
              new_lst.append(i)
      return new_lst

  def remove_stopwords(self,lst):
      stop=stopwords.words('english')
      new_lst=[]
      for i in lst:
          if i not in stop:
              new_lst.append(i)
      return new_lst
  
  def remove_spaces(self,lst):
    new_lst=[]
    for i in lst:
        i=i.strip()
        new_lst.append(i)
    return new_lst

  
  def lemmatzation(self, lst):
    lemmatizer=nltk.stem.WordNetLemmatizer()
    new_lst=[]
    for i in lst:
        i=lemmatizer.lemmatize(i)
        new_lst.append(i)
    return new_lst
    
  def predict(self,text):
    multinomial1 = cloudpickle.load(urlopen("https://github.com/yadnyshree/ML_Assignments/blob/main/ClickBait.sav"))
    tfidf =  cloudpickle.load(urlopen("https://github.com/yadnyshree/ML_Assignments/blob/main/tfidf.sav"))
    dfrme = pd.DataFrame(index=[0], columns=['text'])
    dfrme['text'] = text
    predict=dfrme['text'].apply(self.tokenization)
    predict=predict.apply(self.lowercasing)
    predict=predict.apply(self.remove_punctuations)
    predict=predict.apply(self.remove_stopwords)
    predict=predict.apply(self.remove_spaces)
    predict=predict.apply(self.lemmatzation)
    predict =predict.apply(lambda x: ''.join(i+' ' for i in x))
    text = tfidf.transform(predict)
    train_arr=text.toarray()
    probValue = multinomial1.predict_proba(train_arr)[:,1][0]
    return probValue
"""Title vs Body"""

import sys
import re
import numpy as np
import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('vader_lexicon')

import dill
import pickle
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd
import numpy as np
import seaborn as sns
import random
from collections import Counter
import math
import xgboost as xgb
import lightgbm as lgb

english_stemmer = nltk.stem.SnowballStemmer('english')
token_pattern = r"(?u)\b\w\w+\b"
stopwords = set(nltk.corpus.stopwords.words('english'))


def stem_tokens(tokens, stemmer):
    stemmed = []
    for token in tokens:
        stemmed.append(stemmer.stem(token))
    return stemmed

def preprocess_data(line,
                    token_pattern=token_pattern,
                    exclude_stopword=True,
                    stem=True):
    # token_pattern = re.compile(token_pattern, flags = re.UNICODE | re.LOCALE)
    token_pattern = re.compile(token_pattern, flags = re.UNICODE)
    tokens = [x.lower() for x in token_pattern.findall(line)]
    tokens_stemmed = tokens
    if stem:
        tokens_stemmed = stem_tokens(tokens, english_stemmer)
    if exclude_stopword:
        tokens_stemmed = [x for x in tokens_stemmed if x not in stopwords]

    return tokens_stemmed

def try_divide(x, y, val=0.0):
    """ 
        Try to divide two numbers
    """
    if y != 0.0:
        val = float(x) / y
    return val


def cosine_sim(x, y):
    try:
        if type(x) is np.ndarray: x = x.reshape(1, -1) # get rid of the warning
        if type(y) is np.ndarray: y = y.reshape(1, -1)
        d = cosine_similarity(x, y)
        d = d[0][0]
    except:
        #print(x)
        #print(y)
        d = 0.
    return d